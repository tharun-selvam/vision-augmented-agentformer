#!/usr/bin/env python
"""
Smart BEV feature precomputation - only processes samples actually used by AgentFormer.

This script reads the token list generated by collect_used_samples.py and only
precomputes BEV features for those specific samples, avoiding wasted computation.

Expected speedup: ~27x less samples to process vs. full precomputation

Usage:
    # First, collect used samples
    python scripts/collect_used_samples.py --cfg nuscenes_5sample_agentformer_pre_bev --split train

    # Then run smart precomputation
    python scripts/precompute_bev_features_smart.py --split train --gpu 0 --batch_size 16
"""

import os
import sys
import argparse
import torch
from torch.utils.data import DataLoader, Subset
from pathlib import Path
from tqdm import tqdm
import time
import json

# Add parent directory to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from bevdepth.datasets.nusc_det_dataset import NuscDetDataset
from model.bev.base_lss_fpn import BaseLSSFPN
from utils.config import Config


def load_bev_encoder(cfg, device):
    """Load and freeze BEV encoder"""
    print("Loading BEV encoder...")
    bev_encoder = BaseLSSFPN(**cfg.bev_encoder)

    # Freeze all parameters
    for param in bev_encoder.parameters():
        param.requires_grad = False

    # Set to eval mode
    bev_encoder.eval()
    bev_encoder.to(device)

    print(f"BEV encoder loaded and frozen. Output channels: {bev_encoder.output_channels}")
    return bev_encoder


def load_used_tokens(split, token_file=None):
    """Load the list of sample tokens that are actually used"""
    if token_file is None:
        token_file = f'bev_features/used_tokens_{split}.json'

    if not os.path.exists(token_file):
        raise FileNotFoundError(
            f"Token file not found: {token_file}\n"
            f"Please run: python scripts/collect_used_samples.py --split {split}"
        )

    print(f"Loading used tokens from: {token_file}")
    with open(token_file, 'r') as f:
        data = json.load(f)

    tokens = data['tokens']
    print(f"Loaded {len(tokens)} tokens for {split} split")
    return set(tokens)


def create_dataset(cfg, split):
    """Create BEV dataset for the given split"""
    data_root = cfg.data_root_nuscenes_pred

    # Determine pkl file
    if split == 'train':
        info_paths = os.path.join(data_root, 'nuscenes_infos_train_subset.pkl')
    elif split == 'val':
        info_paths = os.path.join(data_root, 'nuscenes_infos_val_subset.pkl')
    else:
        raise ValueError(f"Unknown split: {split}")

    if not os.path.exists(info_paths):
        raise FileNotFoundError(f"Dataset file not found: {info_paths}")

    print(f"Creating dataset from: {info_paths}")

    nuscenes_data_root = 'nuscenes'

    dataset = NuscDetDataset(
        ida_aug_conf=cfg.ida_aug_conf,
        bda_aug_conf=cfg.bda_aug_conf,
        classes=cfg.object_classes,
        data_root=nuscenes_data_root,
        info_paths=info_paths,
        is_train=False,
        img_conf=cfg.get('img_conf', {}),
        num_sweeps=1,
        return_depth=True,
    )

    # Store info_paths for later use
    dataset.info_paths = info_paths

    print(f"Dataset created with {len(dataset)} samples")
    return dataset, info_paths


def filter_dataset_by_tokens(dataset, info_paths, used_tokens):
    """
    Filter dataset to only include samples with tokens in used_tokens.
    Returns a mapping from filtered index to original index.
    """
    print("Filtering dataset to only used samples...")

    # Read tokens directly from pickle file (fast, no image loading)
    import pickle

    print(f"Reading tokens from pickle: {info_paths}")
    with open(info_paths, 'rb') as f:
        data_infos = pickle.load(f)

    # Create mapping: token -> original index
    token_to_idx = {}
    for idx, info in enumerate(tqdm(data_infos, desc="Building token index")):
        sample_token = info['sample_token']
        token_to_idx[sample_token] = idx

    # Get indices for used tokens
    used_indices = []
    for token in used_tokens:
        if token in token_to_idx:
            used_indices.append(token_to_idx[token])
        else:
            print(f"Warning: Token {token} not found in dataset")

    print(f"Filtered: {len(dataset)} -> {len(used_indices)} samples ({100*len(used_indices)/len(dataset):.1f}%)")
    return used_indices


def collate_fn(batch):
    """Custom collate function for batching BEV data"""
    sweep_imgs = torch.stack([item[0] for item in batch], dim=0)
    sensor2ego_mats = torch.stack([item[1] for item in batch], dim=0)
    intrins = torch.stack([item[2] for item in batch], dim=0)
    ida_mats = torch.stack([item[3] for item in batch], dim=0)
    sensor2sensor_mats = torch.stack([item[4] for item in batch], dim=0)
    bda_mats = torch.stack([item[5] for item in batch], dim=0)
    timestamps = torch.stack([item[6] for item in batch], dim=0)
    img_metas = [item[7] for item in batch]

    return (sweep_imgs, sensor2ego_mats, intrins, ida_mats,
            sensor2sensor_mats, bda_mats, timestamps, img_metas)


def extract_features_smart(bev_encoder, dataset, output_dir, device, batch_size=16, num_workers=8):
    """Extract BEV features using batched processing"""
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    print(f"\nExtracting BEV features to: {output_dir}")
    print(f"Total samples to process: {len(dataset)}")
    print(f"Batch size: {batch_size}")
    print(f"Num workers: {num_workers}")

    # Create DataLoader
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        prefetch_factor=2,
        collate_fn=collate_fn,
        drop_last=False,
    )

    # Statistics
    total_size_mb = 0
    skipped = 0
    extracted = 0
    start_time = time.time()
    batch_times = []

    print(f"\nStarting extraction...")
    print(f"Note: Processing 3 cameras per sample, so each batch processes 48 images")
    print(f"WARNING: First batch may take 5-10 minutes to load (spawning {num_workers} workers + loading images)")
    print(f"         Please be patient... Once first batch completes, rest will be fast!\n")

    with torch.no_grad():
        pbar = tqdm(dataloader, desc="Extracting features", unit="batch")
        for batch_idx, batch_data in enumerate(pbar):
            (sweep_imgs, sensor2ego_mats, intrins, ida_mats,
             sensor2sensor_mats, bda_mats, timestamps, img_metas_list) = batch_data

            current_batch_size = len(img_metas_list)

            # Check which samples already exist
            skip_mask = []
            sample_tokens = []
            for img_metas in img_metas_list:
                sample_token = img_metas['token']
                sample_tokens.append(sample_token)
                output_file = output_dir / f"{sample_token}.pt"
                skip_mask.append(output_file.exists())

            # Skip entire batch if all exist
            if all(skip_mask):
                skipped += current_batch_size
                continue

            # Move to device
            sweep_imgs = sweep_imgs.to(device, non_blocking=True)

            # Add num_sweeps dimension
            if len(sweep_imgs.shape) == 5:
                sweep_imgs = sweep_imgs.unsqueeze(1)

            # Prepare mats_dict
            mats_dict = {
                'sensor2ego_mats': sensor2ego_mats.to(device, non_blocking=True),
                'intrin_mats': intrins.to(device, non_blocking=True),
                'ida_mats': ida_mats.to(device, non_blocking=True),
                'sensor2sensor_mats': sensor2sensor_mats.to(device, non_blocking=True),
                'bda_mat': bda_mats.to(device, non_blocking=True),
            }

            # Add sweep dimension for camera matrices
            for k, v in mats_dict.items():
                if k != 'bda_mat' and len(v.shape) == 4:
                    mats_dict[k] = v.unsqueeze(1)

            # Add sweep dimension to timestamps
            timestamps = timestamps.to(device, non_blocking=True)
            if len(timestamps.shape) == 2:
                timestamps = timestamps.unsqueeze(1)

            # Extract BEV features with mixed precision
            try:
                batch_start = time.time()

                with torch.amp.autocast('cuda'):
                    bev_feature_maps = bev_encoder(sweep_imgs, mats_dict, timestamps=timestamps, is_return_depth=False)

                # Move to CPU and save
                bev_feature_maps = bev_feature_maps.cpu()

                for i, (sample_token, skip) in enumerate(zip(sample_tokens, skip_mask)):
                    if skip:
                        skipped += 1
                        continue

                    output_file = output_dir / f"{sample_token}.pt"
                    torch.save(bev_feature_maps[i], output_file)

                    file_size_mb = output_file.stat().st_size / (1024 * 1024)
                    total_size_mb += file_size_mb
                    extracted += 1

                # Track timing
                batch_time = time.time() - batch_start
                batch_times.append(batch_time)

                # Update progress bar with detailed stats
                if len(batch_times) > 0:
                    avg_batch_time = sum(batch_times[-10:]) / len(batch_times[-10:])  # Last 10 batches
                    samples_per_sec = current_batch_size / avg_batch_time
                    remaining_batches = len(dataloader) - batch_idx - 1
                    eta_seconds = remaining_batches * avg_batch_time
                    eta_minutes = eta_seconds / 60

                    pbar.set_postfix({
                        'extracted': extracted,
                        'batch_time': f'{batch_time:.1f}s',
                        'samples/s': f'{samples_per_sec:.2f}',
                        'ETA': f'{eta_minutes:.1f}m'
                    })

            except Exception as e:
                print(f"\nError processing batch {batch_idx}: {e}")
                import traceback
                traceback.print_exc()
                continue

    elapsed_time = time.time() - start_time
    samples_per_sec = extracted / elapsed_time if elapsed_time > 0 else 0

    print(f"\n{'='*60}")
    print(f"Extraction complete!")
    print(f"  Extracted: {extracted} samples")
    print(f"  Skipped (already exist): {skipped} samples")
    print(f"  Total time: {elapsed_time/3600:.2f} hours ({elapsed_time/60:.2f} minutes)")
    print(f"  Throughput: {samples_per_sec:.2f} samples/second")
    print(f"  Total disk usage: {total_size_mb:.2f} MB ({total_size_mb/1024:.2f} GB)")
    print(f"  Average size per sample: {total_size_mb/max(extracted,1):.2f} MB")
    print(f"{'='*60}")


def main():
    parser = argparse.ArgumentParser(description='Smart BEV precomputation (only used samples)')
    parser.add_argument('--cfg', type=str, default='nuscenes_5sample_agentformer_pre_bev',
                       help='Config name (without .yml)')
    parser.add_argument('--split', type=str, default='train',
                       choices=['train', 'val'],
                       help='Dataset split to process')
    parser.add_argument('--gpu', type=int, default=0,
                       help='GPU device ID')
    parser.add_argument('--batch_size', type=int, default=16,
                       help='Batch size (recommend 16 or 24)')
    parser.add_argument('--num_workers', type=int, default=8,
                       help='Number of data loading workers (8-16 recommended, high values slow first batch)')
    parser.add_argument('--output_dir', type=str, default='bev_features',
                       help='Output directory')
    parser.add_argument('--token_file', type=str, default=None,
                       help='Path to token file (default: bev_features/used_tokens_{split}.json)')

    args = parser.parse_args()

    # Set device
    device = torch.device(f'cuda:{args.gpu}' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(args.gpu)}")
        print(f"Total VRAM: {torch.cuda.get_device_properties(args.gpu).total_memory / 1024**3:.2f} GB")

    # Load config
    print(f"Loading config: {args.cfg}")
    cfg = Config(args.cfg, tmp=False, create_dirs=False)

    # Load used tokens
    used_tokens = load_used_tokens(args.split, args.token_file)

    # Create dataset
    full_dataset, info_paths = create_dataset(cfg, args.split)

    # Filter to only used samples
    used_indices = filter_dataset_by_tokens(full_dataset, info_paths, used_tokens)
    filtered_dataset = Subset(full_dataset, used_indices)

    print(f"\nProcessing {len(filtered_dataset)} samples (vs {len(full_dataset)} in full dataset)")

    # Load BEV encoder
    bev_encoder = load_bev_encoder(cfg, device)

    # Create output directory
    output_dir = os.path.join(args.output_dir, args.split)

    # Extract features
    extract_features_smart(bev_encoder, filtered_dataset, output_dir, device,
                          batch_size=args.batch_size, num_workers=args.num_workers)

    print("\nDone! You can now use these pre-computed features for training.")
    print(f"Set use_precomputed_bev: true in your config to use them.")


if __name__ == '__main__':
    main()
